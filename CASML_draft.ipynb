{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in c:\\users\\chres\\documents\\projects\\genai\\.venv\\lib\\site-packages (3.0.1)\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\chres\\documents\\projects\\genai\\.venv\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.67.1\n"
     ]
    }
   ],
   "source": [
    "!pip install PyPDF2 tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 753/753 [00:17<00:00, 43.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF text extraction complete. Text saved to 'Data/Psychology2e_WEB.txt'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "from tqdm import tqdm\n",
    "reader = PdfReader(\"Data/Psychology2e_WEB.pdf\")\n",
    "\n",
    "text = \"\"\n",
    "for page in tqdm(reader.pages):\n",
    "    text += page.extract_text() + \"\\n\"\n",
    "\n",
    "with open(\"Data/Psychology2e_WEB.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(text)\n",
    "print(\"PDF text extraction complete. Text saved to 'Data/Psychology2e_WEB.txt'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pdfminer.six\n",
      "  Downloading pdfminer_six-20250506-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting charset-normalizer>=2.0.0 (from pdfminer.six)\n",
      "  Using cached charset_normalizer-3.4.2-cp311-cp311-win_amd64.whl.metadata (36 kB)\n",
      "Collecting cryptography>=36.0.0 (from pdfminer.six)\n",
      "  Downloading cryptography-45.0.4-cp311-abi3-win_amd64.whl.metadata (5.7 kB)\n",
      "Collecting cffi>=1.14 (from cryptography>=36.0.0->pdfminer.six)\n",
      "  Using cached cffi-1.17.1-cp311-cp311-win_amd64.whl.metadata (1.6 kB)\n",
      "Collecting pycparser (from cffi>=1.14->cryptography>=36.0.0->pdfminer.six)\n",
      "  Using cached pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Downloading pdfminer_six-20250506-py3-none-any.whl (5.6 MB)\n",
      "   ---------------------------------------- 0.0/5.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 5.6/5.6 MB 34.3 MB/s eta 0:00:00\n",
      "Using cached charset_normalizer-3.4.2-cp311-cp311-win_amd64.whl (105 kB)\n",
      "Downloading cryptography-45.0.4-cp311-abi3-win_amd64.whl (3.4 MB)\n",
      "   ---------------------------------------- 0.0/3.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 3.4/3.4 MB 67.0 MB/s eta 0:00:00\n",
      "Using cached cffi-1.17.1-cp311-cp311-win_amd64.whl (181 kB)\n",
      "Using cached pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Installing collected packages: pycparser, charset-normalizer, cffi, cryptography, pdfminer.six\n",
      "\n",
      "   ---------------------------------------- 0/5 [pycparser]\n",
      "   ---------------------------------------- 0/5 [pycparser]\n",
      "   ---------------------------------------- 0/5 [pycparser]\n",
      "   -------- ------------------------------- 1/5 [charset-normalizer]\n",
      "   -------- ------------------------------- 1/5 [charset-normalizer]\n",
      "   -------- ------------------------------- 1/5 [charset-normalizer]\n",
      "   -------- ------------------------------- 1/5 [charset-normalizer]\n",
      "   ---------------- ----------------------- 2/5 [cffi]\n",
      "   ---------------- ----------------------- 2/5 [cffi]\n",
      "   ---------------- ----------------------- 2/5 [cffi]\n",
      "   ------------------------ --------------- 3/5 [cryptography]\n",
      "   ------------------------ --------------- 3/5 [cryptography]\n",
      "   ------------------------ --------------- 3/5 [cryptography]\n",
      "   ------------------------ --------------- 3/5 [cryptography]\n",
      "   ------------------------ --------------- 3/5 [cryptography]\n",
      "   ------------------------ --------------- 3/5 [cryptography]\n",
      "   ------------------------ --------------- 3/5 [cryptography]\n",
      "   ------------------------ --------------- 3/5 [cryptography]\n",
      "   ------------------------ --------------- 3/5 [cryptography]\n",
      "   ------------------------ --------------- 3/5 [cryptography]\n",
      "   ------------------------ --------------- 3/5 [cryptography]\n",
      "   ------------------------ --------------- 3/5 [cryptography]\n",
      "   -------------------------------- ------- 4/5 [pdfminer.six]\n",
      "   -------------------------------- ------- 4/5 [pdfminer.six]\n",
      "   -------------------------------- ------- 4/5 [pdfminer.six]\n",
      "   -------------------------------- ------- 4/5 [pdfminer.six]\n",
      "   -------------------------------- ------- 4/5 [pdfminer.six]\n",
      "   -------------------------------- ------- 4/5 [pdfminer.six]\n",
      "   -------------------------------- ------- 4/5 [pdfminer.six]\n",
      "   -------------------------------- ------- 4/5 [pdfminer.six]\n",
      "   -------------------------------- ------- 4/5 [pdfminer.six]\n",
      "   -------------------------------- ------- 4/5 [pdfminer.six]\n",
      "   ---------------------------------------- 5/5 [pdfminer.six]\n",
      "\n",
      "Successfully installed cffi-1.17.1 charset-normalizer-3.4.2 cryptography-45.0.4 pdfminer.six-20250506 pycparser-2.22\n"
     ]
    }
   ],
   "source": [
    "!pip install pdfminer.six\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text with pdfminer...\n",
      "PDF text extraction complete. Text saved to 'Data/Psychology2e_WEB.txt'.\n"
     ]
    }
   ],
   "source": [
    "from pdfminer.high_level import extract_text\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "pdf_path = \"Data/Psychology2e_WEB.pdf\"\n",
    "txt_path = \"Data/Psychology2e_WEB.txt\"\n",
    "\n",
    "# Extract all text from PDF\n",
    "print(\"Extracting text with pdfminer...\")\n",
    "text = extract_text(pdf_path)\n",
    "\n",
    "# Save to .txt file\n",
    "os.makedirs(os.path.dirname(txt_path), exist_ok=True)\n",
    "with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(text)\n",
    "\n",
    "print(f\"PDF text extraction complete. Text saved to '{txt_path}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text with PyPDF2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 753/753 [00:17<00:00, 44.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyPDF2 output to psych_text_pypdf2.txt\n",
      "Extracting text with pdfminer...\n",
      "Saved pdfminer output to psych_text_pdfminer.txt\n",
      "Comparing extracted texts...\n",
      "Comparison saved to text_diff.txt\n"
     ]
    }
   ],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "from pdfminer.high_level import extract_text\n",
    "from tqdm import tqdm\n",
    "import difflib\n",
    "import os\n",
    "\n",
    "# ----------- Step 1: Extract with PyPDF2 -----------\n",
    "print(\"Extracting text with PyPDF2...\")\n",
    "reader = PdfReader(\"Data/Psychology2e_WEB.pdf\")\n",
    "text1 = \"\"\n",
    "for page in tqdm(reader.pages):\n",
    "    text1 += page.extract_text() + \"\\n\"\n",
    "\n",
    "# Save PyPDF2 output\n",
    "with open(\"Data/psych_text_pypdf2.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(text1)\n",
    "print(\"Saved PyPDF2 output to psych_text_pypdf2.txt\")\n",
    "\n",
    "# ----------- Step 2: Extract with pdfminer -----------\n",
    "print(\"Extracting text with pdfminer...\")\n",
    "text2 = extract_text(\"Data/Psychology2e_WEB.pdf\")\n",
    "\n",
    "# Save pdfminer output\n",
    "with open(\"Data/psych_text_pdfminer.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(text2)\n",
    "print(\"Saved pdfminer output to psych_text_pdfminer.txt\")\n",
    "\n",
    "# ----------- Step 3: Compare Texts -----------\n",
    "print(\"Comparing extracted texts...\")\n",
    "diff = difflib.unified_diff(\n",
    "    text1.splitlines(),\n",
    "    text2.splitlines(),\n",
    "    fromfile='PyPDF2',\n",
    "    tofile='pdfminer',\n",
    "    lineterm=''\n",
    ")\n",
    "\n",
    "with open(\"text_diff.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in diff:\n",
    "        f.write(line + \"\\n\")\n",
    "\n",
    "print(\"Comparison saved to text_diff.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "753it [00:22, 33.40it/s]\n"
     ]
    }
   ],
   "source": [
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from io import StringIO\n",
    "from tqdm import tqdm\n",
    "\n",
    "# high_level import extract_text\n",
    "\n",
    "# Specify the path to your PDF file\n",
    "pdf_path = \"Data/Psychology2e_WEB.pdf\"\n",
    "\n",
    "rsrc_mgr = PDFResourceManager()\n",
    "retstr = StringIO()\n",
    "laparams = LAParams()\n",
    "device = TextConverter(rsrc_mgr, retstr, laparams=laparams)\n",
    "interpreter = PDFPageInterpreter(rsrc_mgr, device)\n",
    "\n",
    "# Open the PDF file\n",
    "with open(pdf_path, \"rb\") as fp:\n",
    "    # Process each page in the PDF\n",
    "\n",
    "    # The first 18 pages are not needed, so we will skip them\n",
    "    # References are also not needed. So we will skip these as well\n",
    "    page_number = 0\n",
    "    for page in tqdm(PDFPage.get_pages(fp)):\n",
    "        page_number += 1\n",
    "        if page_number <= 18 or page_number > 644:\n",
    "            continue\n",
    "        interpreter.process_page(page)\n",
    "    # Get the text from the StringIO object\n",
    "    text = retstr.getvalue()\n",
    "# Close the device and StringIO object\n",
    "device.close()\n",
    "retstr.close()\n",
    "\n",
    "# Print or save the extracted text\n",
    "text = text.replace(\"\\n\\nAccess for free at openstax.org\", \"\") \n",
    "# Optionally, save the text to a file\n",
    "with open(\"Data/Psychology2e_WEB_pdfminer_trimmed.txt\", \"w\", encoding=\"utf-8\") as text_file:\n",
    "    text_file.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "with open(\"Data/Psychology2e_WEB_pdfminer_trimmed.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    full_text = f.read()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500, chunk_overlap=50, separators=[\"\\n\\n\", \"\\n\", \".\", \" \"]\n",
    ")\n",
    "chunks = splitter.create_documents([full_text])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4269"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={}, page_content='Introduction to Psychology\\n\\n1\\n\\nFIGURE 1.1 Psychology is the scientific study of mind and behavior. (credit \"background\": modification of work by\\n\\nNattachai Noogure; credit \"top left\": modification of work by Peter Shanks; credit \"top middle\": modification of work\\n\\nby \"devinf\"/Flickr; credit \"top right\": modification of work by Alejandra Quintero Sinisterra; credit \"bottom left\":\\n\\nmodification of work by Gabriel Rocha; credit \"bottom middle-left\": modification of work by Caleb Roenigk; credit')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pinecone\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.embeddings import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "pinecone_api_key = \"pcsk_4cC2jC_DYQseHxA8jge2oFn6o7SV5F6aMYJqWXuQzUTJpqzhwyiHLyahNYqvobky7emeoT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "\n",
    "index_name = \"casml-py\"\n",
    "# Initialize embedding model\n",
    "if not pc.has_index(index_name):\n",
    "    pc.create_index(\n",
    "        name=index_name,  \n",
    "        dimension=384,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading batches: 100%|██████████| 43/43 [00:50<00:00,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully upserted 4269 chunks to Pinecone index 'casml-py'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Connect to Pinecone\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "batch_size = 100\n",
    "for i in tqdm(range(0, len(chunks), batch_size), desc=\"Uploading batches\"):\n",
    "    batch = chunks[i:i + batch_size]\n",
    "    texts = [doc.page_content for doc in batch]\n",
    "    ids = [f\"chunk-{i+j}\" for j in range(len(batch))]\n",
    "    vectors = embedder.encode(texts).tolist()\n",
    "\n",
    "    to_upsert = [\n",
    "        {\"id\": ids[j], \"values\": vectors[j], \"metadata\": {\"text\": texts[j]}}\n",
    "        for j in range(len(batch))\n",
    "    ]\n",
    "    index.upsert(vectors=to_upsert)\n",
    "\n",
    "print(f\"✅ Successfully upserted {len(chunks)} chunks to Pinecone index '{index_name}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📌 Score: 0.736447394\n",
      "psychology is the area of psychology that focuses on studying cognitions, or thoughts, and their relationship to\n",
      "\n",
      "our experiences and our actions. Like biological psychology, cognitive psychology is broad in its scope and\n",
      "\n",
      "often involves collaborations among people from a diverse range of disciplinary backgrounds. This has led\n",
      "\n",
      "some to coin the term cognitive science to describe the interdisciplinary nature of this area of research (Miller,\n",
      "\n",
      "2003).\n",
      "\n",
      "📌 Score: 0.720964491\n",
      "explores questions like these. Psychology refers to the scientific study of the mind and behavior. Psychologists\n",
      "\n",
      "use the scientific method to acquire knowledge. To apply the scientific method, a researcher with a question\n",
      "\n",
      "about how or why something happens will propose a tentative explanation, called a hypothesis, to explain the\n",
      "\n",
      "phenomenon. A hypothesis should fit into the context of a scientific theory, which is a broad explanation or\n",
      "\n",
      "📌 Score: 0.649055541\n",
      "disorders and other problematic patterns of behavior. As such, it is generally considered to be a more applied\n",
      "\n",
      "area within psychology; however, some clinicians are also actively engaged in scientific research. Counseling\n",
      "\n",
      "psychology is a similar discipline that focuses on emotional, social, vocational, and health-related outcomes in\n",
      "\n",
      "individuals who are considered psychologically healthy.\n",
      "\n",
      "📌 Score: 0.636596262\n",
      "12.1 • What Is Social Psychology? 401\n",
      "\n",
      "interpersonal processes of conformity and obedience, aggression and altruism, and, finally, love and\n",
      "\n",
      "attraction.\n",
      "\n",
      "Situational and Dispositional Influences on Behavior\n",
      "\n",
      "Behavior is a product of both the situation (e.g., cultural influences, social roles, and the presence of\n",
      "\n",
      "bystanders) and of the person (e.g., personality characteristics). Subfields of psychology tend to focus on one\n",
      "\n",
      "📌 Score: 0.636428416\n",
      "organism exists in isolation, and our behavior is influenced by our interactions with others. Therefore,\n",
      "\n",
      "psychology is also a social science.\n",
      "\n",
      "WHY STUDY PSYCHOLOGY?\n",
      "\n",
      "Often, students take their first psychology course because they are interested in helping others and want to\n",
      "\n",
      "learn more about themselves and why they act the way they do. Sometimes, students take a psychology course\n",
      "\n",
      "because it either satisfies a general education requirement or is required for a program of study such as\n"
     ]
    }
   ],
   "source": [
    "query = \"What is psychology?\"\n",
    "query_vector = embedder.encode([query]).tolist()\n",
    "\n",
    "# Search top 5 similar chunks\n",
    "results = index.query(vector=query_vector[0], top_k=5, include_metadata=True)\n",
    "\n",
    "for match in results['matches']:\n",
    "    print(f\"\\n📌 Score: {match['score']}\")\n",
    "    print(match['metadata']['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting pages: 369it [01:56,  3.16it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m i <= \u001b[32m18\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m i > \u001b[32m644\u001b[39m:\n\u001b[32m     11\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m text = \u001b[43mextract_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpage_numbers\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 0-based\u001b[39;00m\n\u001b[32m     13\u001b[39m text = text.replace(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mAccess for free at openstax.org\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m page_texts.append((i, text))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chres\\Documents\\Projects\\genai\\.venv\\Lib\\site-packages\\pdfminer\\high_level.py:177\u001b[39m, in \u001b[36mextract_text\u001b[39m\u001b[34m(pdf_file, password, page_numbers, maxpages, caching, codec, laparams)\u001b[39m\n\u001b[32m    174\u001b[39m device = TextConverter(rsrcmgr, output_string, codec=codec, laparams=laparams)\n\u001b[32m    175\u001b[39m interpreter = PDFPageInterpreter(rsrcmgr, device)\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mPDFPage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_pages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpage_numbers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaxpages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaxpages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcaching\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcaching\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterpreter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess_page\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output_string.getvalue()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chres\\Documents\\Projects\\genai\\.venv\\Lib\\site-packages\\pdfminer\\pdfpage.py:176\u001b[39m, in \u001b[36mPDFPage.get_pages\u001b[39m\u001b[34m(cls, fp, pagenos, maxpages, password, caching, check_extractable)\u001b[39m\n\u001b[32m    174\u001b[39m         log.warning(warning_msg)\n\u001b[32m    175\u001b[39m \u001b[38;5;66;03m# Process each page contained in the document.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpageno\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcreate_pages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpagenos\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mpageno\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpagenos\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mcontinue\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chres\\Documents\\Projects\\genai\\.venv\\Lib\\site-packages\\pdfminer\\pdfpage.py:133\u001b[39m, in \u001b[36mPDFPage.create_pages\u001b[39m\u001b[34m(cls, document)\u001b[39m\n\u001b[32m    131\u001b[39m     objects = depth_first_search(document.catalog[\u001b[33m\"\u001b[39m\u001b[33mPages\u001b[39m\u001b[33m\"\u001b[39m], document.catalog)\n\u001b[32m    132\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m objid, tree \u001b[38;5;129;01min\u001b[39;00m objects:\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdocument\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobjid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtree\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpage_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    134\u001b[39m         pages = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pages:\n\u001b[32m    136\u001b[39m     \u001b[38;5;66;03m# fallback when /Pages is missing.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chres\\Documents\\Projects\\genai\\.venv\\Lib\\site-packages\\pdfminer\\pdfpage.py:67\u001b[39m, in \u001b[36mPDFPage.__init__\u001b[39m\u001b[34m(self, doc, pageid, attrs, label)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28mself\u001b[39m.label = label\n\u001b[32m     66\u001b[39m \u001b[38;5;28mself\u001b[39m.lastmod = resolve1(\u001b[38;5;28mself\u001b[39m.attrs.get(\u001b[33m\"\u001b[39m\u001b[33mLastModified\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m \u001b[38;5;28mself\u001b[39m.resources: Dict[\u001b[38;5;28mobject\u001b[39m, \u001b[38;5;28mobject\u001b[39m] = \u001b[43mresolve1\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mResources\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[38;5;28mself\u001b[39m.mediabox = \u001b[38;5;28mself\u001b[39m._parse_mediabox(\u001b[38;5;28mself\u001b[39m.attrs.get(\u001b[33m\"\u001b[39m\u001b[33mMediaBox\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m     72\u001b[39m \u001b[38;5;28mself\u001b[39m.cropbox = \u001b[38;5;28mself\u001b[39m._parse_cropbox(\u001b[38;5;28mself\u001b[39m.attrs.get(\u001b[33m\"\u001b[39m\u001b[33mCropBox\u001b[39m\u001b[33m\"\u001b[39m), \u001b[38;5;28mself\u001b[39m.mediabox)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chres\\Documents\\Projects\\genai\\.venv\\Lib\\site-packages\\pdfminer\\pdftypes.py:117\u001b[39m, in \u001b[36mresolve1\u001b[39m\u001b[34m(x, default)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Resolves an object.\u001b[39;00m\n\u001b[32m    112\u001b[39m \n\u001b[32m    113\u001b[39m \u001b[33;03mIf this is an array or dictionary, it may still contains\u001b[39;00m\n\u001b[32m    114\u001b[39m \u001b[33;03msome indirect objects inside.\u001b[39;00m\n\u001b[32m    115\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, PDFObjRef):\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     x = \u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdefault\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chres\\Documents\\Projects\\genai\\.venv\\Lib\\site-packages\\pdfminer\\pdftypes.py:105\u001b[39m, in \u001b[36mPDFObjRef.resolve\u001b[39m\u001b[34m(self, default)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.doc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetobj\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mobjid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m PDFObjectNotFound:\n\u001b[32m    107\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m default\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chres\\Documents\\Projects\\genai\\.venv\\Lib\\site-packages\\pdfminer\\pdfdocument.py:861\u001b[39m, in \u001b[36mPDFDocument.getobj\u001b[39m\u001b[34m(self, objid)\u001b[39m\n\u001b[32m    859\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    860\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m strmid \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m861\u001b[39m         stream = stream_value(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgetobj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstrmid\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    862\u001b[39m         obj = \u001b[38;5;28mself\u001b[39m._getobj_objstm(stream, index, objid)\n\u001b[32m    863\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chres\\Documents\\Projects\\genai\\.venv\\Lib\\site-packages\\pdfminer\\pdfdocument.py:864\u001b[39m, in \u001b[36mPDFDocument.getobj\u001b[39m\u001b[34m(self, objid)\u001b[39m\n\u001b[32m    862\u001b[39m     obj = \u001b[38;5;28mself\u001b[39m._getobj_objstm(stream, index, objid)\n\u001b[32m    863\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m864\u001b[39m     obj = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getobj_parse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobjid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    865\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.decipher:\n\u001b[32m    866\u001b[39m         obj = decipher_all(\u001b[38;5;28mself\u001b[39m.decipher, objid, genno, obj)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chres\\Documents\\Projects\\genai\\.venv\\Lib\\site-packages\\pdfminer\\pdfdocument.py:817\u001b[39m, in \u001b[36mPDFDocument._getobj_parse\u001b[39m\u001b[34m(self, pos, objid)\u001b[39m\n\u001b[32m    815\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._parser \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    816\u001b[39m \u001b[38;5;28mself\u001b[39m._parser.seek(pos)\n\u001b[32m--> \u001b[39m\u001b[32m817\u001b[39m (_, objid1) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_parser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnexttoken\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# objid\u001b[39;00m\n\u001b[32m    818\u001b[39m (_, genno) = \u001b[38;5;28mself\u001b[39m._parser.nexttoken()  \u001b[38;5;66;03m# genno\u001b[39;00m\n\u001b[32m    819\u001b[39m (_, kwd) = \u001b[38;5;28mself\u001b[39m._parser.nexttoken()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chres\\Documents\\Projects\\genai\\.venv\\Lib\\site-packages\\pdfminer\\psparser.py:509\u001b[39m, in \u001b[36mPSBaseParser.nexttoken\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    507\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tokens:\n\u001b[32m    508\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m509\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfillbuf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    510\u001b[39m         \u001b[38;5;28mself\u001b[39m.charpos = \u001b[38;5;28mself\u001b[39m._parse1(\u001b[38;5;28mself\u001b[39m.buf, \u001b[38;5;28mself\u001b[39m.charpos)\n\u001b[32m    511\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m PSEOF:\n\u001b[32m    512\u001b[39m         \u001b[38;5;66;03m# If we hit EOF in the middle of a token, try to parse\u001b[39;00m\n\u001b[32m    513\u001b[39m         \u001b[38;5;66;03m# it by tacking on whitespace, and delay raising PSEOF\u001b[39;00m\n\u001b[32m    514\u001b[39m         \u001b[38;5;66;03m# until next time around\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chres\\Documents\\Projects\\genai\\.venv\\Lib\\site-packages\\pdfminer\\psparser.py:214\u001b[39m, in \u001b[36mPSBaseParser.fillbuf\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    212\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    213\u001b[39m \u001b[38;5;66;03m# fetch next chunk.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m \u001b[38;5;28mself\u001b[39m.bufpos = \u001b[38;5;28mself\u001b[39m.fp.tell()\n\u001b[32m    215\u001b[39m \u001b[38;5;28mself\u001b[39m.buf = \u001b[38;5;28mself\u001b[39m.fp.read(\u001b[38;5;28mself\u001b[39m.BUFSIZ)\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.buf:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from pdfminer.high_level import extract_text\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from tqdm import tqdm\n",
    "\n",
    "pdf_path = \"Data/Psychology2e_WEB.pdf\"\n",
    "page_texts = []\n",
    "\n",
    "with open(pdf_path, \"rb\") as f:\n",
    "    for i, page in enumerate(tqdm(PDFPage.get_pages(f), desc=\"Extracting pages\"), 1):\n",
    "        if i <= 18 or i > 644:\n",
    "            continue\n",
    "        text = extract_text(pdf_path, page_numbers=[i - 1])  # 0-based\n",
    "        text = text.replace(\"\\n\\nAccess for free at openstax.org\", \"\")\n",
    "        page_texts.append((i, text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting pages: 753it [03:52,  3.24it/s]\n"
     ]
    }
   ],
   "source": [
    "from pdfminer.high_level import extract_text\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "pdf_path = \"Data/Psychology2e_WEB.pdf\"\n",
    "page_texts = []\n",
    "\n",
    "with open(pdf_path, \"rb\") as f:\n",
    "    for i, page in enumerate(tqdm(PDFPage.get_pages(f), desc=\"Extracting pages\"), 1):\n",
    "        text = extract_text(pdf_path, page_numbers=[i - 1])\n",
    "        text = text.replace(\"\\n\\nAccess for free at openstax.org\", \"\")\n",
    "        page_texts.append((i, text))\n",
    "\n",
    "\n",
    "def convert_to_markdown_headers(text):\n",
    "    text = re.sub(r\"\\bCHAPTER\\s+(\\d+)\\s+(.*)\", r\"# \\1 \\2\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"\\n(\\d{1,2}\\.\\d+)\\s+(.*)\", r\"\\n## \\1 \\2\", text)\n",
    "    return text\n",
    "\n",
    "markdown_pages = [(p, convert_to_markdown_headers(t)) for p, t in page_texts]        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
    "\n",
    "header_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[(\"#\", \"chapter\"), (\"##\", \"section\")])\n",
    "docs_with_meta = []\n",
    "\n",
    "for page_num, md_text in markdown_pages:\n",
    "    page_docs = header_splitter.split_text(md_text)\n",
    "    for d in page_docs:\n",
    "        d.metadata[\"page\"] = page_num\n",
    "    docs_with_meta.extend(page_docs)\n",
    "\n",
    "chunk_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "final_chunks = []\n",
    "for doc in docs_with_meta:\n",
    "    chunks = chunk_splitter.create_documents([doc.page_content])\n",
    "    for chunk in chunks:\n",
    "        chunk.metadata.update(doc.metadata)\n",
    "    final_chunks.extend(chunks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "pinecone_api_key = \"pcsk_4cC2jC_DYQseHxA8jge2oFn6o7SV5F6aMYJqWXuQzUTJpqzhwyiHLyahNYqvobky7emeoT\"\n",
    "index_name = \"casml-py\"\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "\n",
    "if not pc.has_index(index_name):\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=384,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "    )\n",
    "\n",
    "index = pc.Index(index_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 'casml-py' has been deleted.\n"
     ]
    }
   ],
   "source": [
    "# Delete the index\n",
    "# if pc.has_index(index_name):\n",
    "#     pc.delete_index(index_name)\n",
    "#     print(f\"Index '{index_name}' has been deleted.\")\n",
    "# else:\n",
    "#     print(f\"Index '{index_name}' does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to Pinecone: 100%|██████████| 57/57 [01:13<00:00,  1.28s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "batch_size = 100\n",
    "for i in tqdm(range(0, len(final_chunks), batch_size), desc=\"Uploading to Pinecone\"):\n",
    "    batch = final_chunks[i:i + batch_size]\n",
    "    texts = [doc.page_content for doc in batch]\n",
    "    ids = [f\"chunk-{i+j}\" for j in range(len(batch))]\n",
    "    vectors = embedder.encode(texts).tolist()\n",
    "    to_upsert = [\n",
    "        {\n",
    "            \"id\": ids[j],\n",
    "            \"values\": vectors[j],\n",
    "            \"metadata\": {\n",
    "                **batch[j].metadata,\n",
    "                \"text\": texts[j]  # ✅ Now included for query results\n",
    "            }\n",
    "        }\n",
    "        for j in range(len(batch))\n",
    "    ]\n",
    "    index.upsert(vectors=to_upsert)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embedder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Your search query\u001b[39;00m\n\u001b[32m      2\u001b[39m query = \u001b[33m\"\u001b[39m\u001b[33mWhat is cognitive psychology?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m query_vector = \u001b[43membedder\u001b[49m.encode([query]).tolist()\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Query Pinecone\u001b[39;00m\n\u001b[32m      6\u001b[39m results = index.query(vector=query_vector[\u001b[32m0\u001b[39m], top_k=\u001b[32m5\u001b[39m, include_metadata=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'embedder' is not defined"
     ]
    }
   ],
   "source": [
    "# Your search query\n",
    "query = \"What is cognitive psychology?\"\n",
    "query_vector = embedder.encode([query]).tolist()\n",
    "\n",
    "# Query Pinecone\n",
    "results = index.query(vector=query_vector[0], top_k=5, include_metadata=True)\n",
    "\n",
    "# Display the results\n",
    "for match in results[\"matches\"]:\n",
    "    print(f\"\\nScore: {match['score']:.4f}\")\n",
    "    print(f\"Page: {match['metadata'].get('page')}\")\n",
    "    print(f\"Chapter: {match['metadata'].get('chapter')}\")\n",
    "    print(f\"Section: {match['metadata'].get('section')}\")\n",
    "    print(f\"\\nText:\\n{match['metadata']['text'][:500]}...\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
